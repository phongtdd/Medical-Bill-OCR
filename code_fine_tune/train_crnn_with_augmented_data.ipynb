{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11957414,"sourceType":"datasetVersion","datasetId":7518100},{"sourceId":12030664,"sourceType":"datasetVersion","datasetId":7569394}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch torchvision torchaudio --quiet\n!pip install albumentations --quiet  # for augmentation (optional)\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nimport torchvision.transforms as transforms\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nimport numpy as np\nfrom PIL import Image\nimport os\nimport string\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T19:36:51.032877Z","iopub.execute_input":"2025-05-23T19:36:51.033088Z","iopub.status.idle":"2025-05-23T19:38:23.122923Z","shell.execute_reply.started":"2025-05-23T19:36:51.033070Z","shell.execute_reply":"2025-05-23T19:38:23.122077Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"batch_size = 32\nlr = 1e-4","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Vietnamese Alphabet","metadata":{}},{"cell_type":"code","source":"# Define Vietnamese characters set (you can expand it)\n# Including a blank character '' for CTC at index 0\nlowercase = \"aăâbcdđeêghijklmnoôơpqrstuưvwxyz\" \\\n            \"áàảãạằắẳẵặấầẩẫậéèẻẽẹếềểễệíìỉĩịóòỏõọốồổỗộớờởỡợúùủũụứừửữựýỳỷỹỵ0123456789\"\n\nuppercase = lowercase.upper()\n\nspecial_chars = \"/!@#$%^&*()_+:,.-;?{}[]|~` \"\n\nfull_alphabet = lowercase + uppercase + special_chars\nprint(full_alphabet)\n# Map char to index and vice versa\nchar_to_idx = {char: idx + 1 for idx, char in enumerate(full_alphabet)}  # start at 1; 0 is blank for CTC\nidx_to_char = {idx: char for char, idx in char_to_idx.items()}\n\n# Add blank character at index 0\nidx_to_char[0] = ''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T22:01:46.240887Z","iopub.execute_input":"2025-05-23T22:01:46.241658Z","iopub.status.idle":"2025-05-23T22:01:46.246733Z","shell.execute_reply.started":"2025-05-23T22:01:46.241634Z","shell.execute_reply":"2025-05-23T22:01:46.245961Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dataset Preparation","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\nclass VietnameseOCRDataset(Dataset):\n    def __init__(self, img_dir, labels_csv, transform=None):\n        self.img_dir = img_dir\n        self.transform = transform\n        df = pd.read_csv(labels_csv, encoding='utf-8')\n        self.samples = list(zip(df['image_name'], df['text']))\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        img_name, label = self.samples[idx]\n        img_path = os.path.join(self.img_dir, img_name)\n        image = Image.open(img_path).convert('L')  # grayscale\n    \n        if self.transform:\n            augmented = self.transform(image=np.array(image))  # pass as named argument\n            image = augmented['image']                        # get transformed image tensor\n    \n        # Encode label string to list of indices\n        label_idx = [char_to_idx[char] for char in label if char in char_to_idx]\n    \n        return image, torch.tensor(label_idx, dtype=torch.long)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T22:01:47.582380Z","iopub.execute_input":"2025-05-23T22:01:47.582840Z","iopub.status.idle":"2025-05-23T22:01:47.588570Z","shell.execute_reply.started":"2025-05-23T22:01:47.582819Z","shell.execute_reply":"2025-05-23T22:01:47.587979Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Define Transformations (Resize + Normalize)","metadata":{}},{"cell_type":"code","source":"transform = A.Compose([\n    A.Resize(32, 512),  # height fixed to 32, width 128 (adjust as needed)\n    A.Normalize(mean=(0.5,), std=(0.5,)),\n    ToTensorV2(),\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T22:42:51.356763Z","iopub.execute_input":"2025-05-23T22:42:51.357284Z","iopub.status.idle":"2025-05-23T22:42:51.362953Z","shell.execute_reply.started":"2025-05-23T22:42:51.357260Z","shell.execute_reply":"2025-05-23T22:42:51.362372Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Instantiate Dataset and DataLoader","metadata":{}},{"cell_type":"code","source":"train_dataset = VietnameseOCRDataset('/kaggle/input/genrated-text/generated_text_recognition/train', '/kaggle/input/genrated-text/generated_text_recognition/train.csv', transform=transform)\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=lambda x: x)\nval_dataset = VietnameseOCRDataset('/kaggle/input/genrated-text/generated_text_recognition/val', '/kaggle/input/genrated-text/generated_text_recognition/val.csv', transform=transform)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, collate_fn=lambda x: x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T22:42:53.118658Z","iopub.execute_input":"2025-05-23T22:42:53.118932Z","iopub.status.idle":"2025-05-23T22:42:53.172639Z","shell.execute_reply.started":"2025-05-23T22:42:53.118913Z","shell.execute_reply":"2025-05-23T22:42:53.172098Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CRNN Model","metadata":{}},{"cell_type":"code","source":"class CRNN(nn.Module):\n    def __init__(self, imgH, nc, nclass, nh):\n        super(CRNN, self).__init__()\n        assert imgH % 16 == 0, \"imgH has to be a multiple of 16\"\n\n        self.cnn = nn.Sequential(\n            nn.Conv2d(nc, 64, 3, 1, 1),  # conv1\n            nn.ReLU(True),\n            nn.MaxPool2d(2, 2),          # 32x128 -> 16x64\n\n            nn.Conv2d(64, 128, 3, 1, 1), # conv2\n            nn.ReLU(True),\n            nn.MaxPool2d(2, 2),          # 16x64 -> 8x32\n\n            nn.Conv2d(128, 256, 3, 1, 1), # conv3\n            nn.ReLU(True),\n            nn.Conv2d(256, 256, 3, 1, 1), # conv4\n            nn.ReLU(True),\n            nn.MaxPool2d((2,2), (2,1), (0,1)), # 8x32 -> 4x33\n\n            nn.Conv2d(256, 512, 3, 1, 1), # conv5\n            nn.BatchNorm2d(512),\n            nn.ReLU(True),\n            nn.Conv2d(512, 512, 3, 1, 1), # conv6\n            nn.BatchNorm2d(512),\n            nn.ReLU(True),\n            nn.MaxPool2d((2,2), (2,1), (0,1)), # 4x33 -> 2x34\n\n            nn.Conv2d(512, 512, 2, 1, 0),  # conv7 kernel=2 no padding\n            nn.ReLU(True)\n        )\n\n        self.rnn = nn.LSTM(\n            input_size=512,\n            hidden_size=nh,\n            num_layers=2,\n            bidirectional=True,\n            batch_first=True\n        )\n\n        self.embedding = nn.Linear(nh * 2, nclass)\n\n    def forward(self, x):\n        # x: (batch, channel=1, height, width)\n        conv = self.cnn(x)  # [batch, 512, 1, width']\n        b, c, h, w = conv.size()\n        assert h == 1, \"height after conv must be 1\"\n        conv = conv.squeeze(2)  # [batch, 512, width]\n        conv = conv.permute(0, 2, 1)  # [batch, width, 512]\n\n        rnn_out, _ = self.rnn(conv)  # [batch, width, nh*2]\n        output = self.embedding(rnn_out)  # [batch, width, nclass]\n\n        # output: logit sequence for CTC loss\n        return output.log_softmax(2)  # for CTC loss: log prob on dim=2\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T22:42:58.531853Z","iopub.execute_input":"2025-05-23T22:42:58.532518Z","iopub.status.idle":"2025-05-23T22:42:58.540060Z","shell.execute_reply.started":"2025-05-23T22:42:58.532498Z","shell.execute_reply":"2025-05-23T22:42:58.539268Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CTC Loss and Optimizer","metadata":{}},{"cell_type":"code","source":"device = torch.device('cuda')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T22:43:02.429278Z","iopub.execute_input":"2025-05-23T22:43:02.429518Z","iopub.status.idle":"2025-05-23T22:43:02.432852Z","shell.execute_reply.started":"2025-05-23T22:43:02.429503Z","shell.execute_reply":"2025-05-23T22:43:02.432314Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = CRNN(imgH=32, nc=1, nclass=len(full_alphabet) + 1, nh=256).to(device)\nctc_loss = nn.CTCLoss(blank=0, zero_infinity=True)\noptimizer = optim.Adam(model.parameters(), lr=lr)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T22:43:02.664523Z","iopub.execute_input":"2025-05-23T22:43:02.665138Z","iopub.status.idle":"2025-05-23T22:43:02.753832Z","shell.execute_reply.started":"2025-05-23T22:43:02.665106Z","shell.execute_reply":"2025-05-23T22:43:02.753098Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Early Stopping","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport torch\n\nclass EarlyStopping:\n    def __init__(self, patience=5, verbose=False, delta=0, path='checkpoint.pth'):\n        \"\"\"\n        Args:\n            patience (int): How many epochs to wait after last improvement.\n            verbose (bool): If True, prints messages when validation loss improves.\n            delta (float): Minimum change to qualify as improvement.\n            path (str): Path to save the best model.\n        \"\"\"\n        self.patience = patience\n        self.verbose = verbose\n        self.delta = delta\n        self.path = path\n\n        self.counter = 0\n        self.best_loss = np.Inf\n        self.early_stop = False\n        self.best_model_state = None\n\n    def __call__(self, val_loss, model):\n        if val_loss < self.best_loss - self.delta:\n            self.best_loss = val_loss\n            self.best_model_state = model.state_dict()\n            self.counter = 0\n            if self.verbose:\n                torch.save(self.best_model_state, self.path)\n        else:\n            self.counter += 1\n            if self.verbose:\n                if self.counter >= self.patience:\n                    self.early_stop = True\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T22:43:03.659723Z","iopub.execute_input":"2025-05-23T22:43:03.660003Z","iopub.status.idle":"2025-05-23T22:43:03.665965Z","shell.execute_reply.started":"2025-05-23T22:43:03.659983Z","shell.execute_reply":"2025-05-23T22:43:03.665232Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\n\ndef validate_epoch(model, dataloader, ctc_loss, device):\n    model.eval()\n    total_loss = 0\n\n    with torch.no_grad():\n        for batch in dataloader:\n            images, labels = zip(*batch)\n            images = torch.stack(images).to(device)\n\n            label_lengths = torch.tensor([len(l) for l in labels], dtype=torch.long)\n            targets = torch.cat(labels).to(device)\n\n            outputs = model(images)\n            outputs = outputs.permute(1, 0, 2)  # (T, N, C)\n\n            input_lengths = torch.full(size=(outputs.size(1),), fill_value=outputs.size(0), dtype=torch.long)\n\n            loss = ctc_loss(outputs, targets, input_lengths, label_lengths)\n            total_loss += loss.item()\n\n    avg_loss = total_loss / len(dataloader)\n    print(f\"Validation loss: {avg_loss:.4f}\")\n    model.train()\n    return avg_loss\n\ndef train_epoch(model, dataloader, optimizer, ctc_loss, device):\n    model.train()\n    total_loss = 0\n    with tqdm(dataloader, unit=\"batch\") as tepoch:\n        for batch in tepoch:\n            images, labels = zip(*batch)\n            images = torch.stack(images).to(device)\n\n            label_lengths = torch.tensor([len(l) for l in labels], dtype=torch.long)\n            targets = torch.cat(labels).to(device)\n\n            outputs = model(images)\n            outputs = outputs.permute(1, 0, 2)\n\n            input_lengths = torch.full(size=(outputs.size(1),), fill_value=outputs.size(0), dtype=torch.long)\n\n            optimizer.zero_grad()\n            loss = ctc_loss(outputs, targets, input_lengths, label_lengths)\n            loss.backward()\n            optimizer.step()\n\n            loss_value = loss.item()\n            total_loss += loss_value\n\n            tepoch.set_postfix(loss=loss_value)\n            \n\n    avg_loss = total_loss / len(dataloader)\n    print(f\"Epoch loss: {avg_loss:.4f}\")\n    return avg_loss\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T22:43:05.274241Z","iopub.execute_input":"2025-05-23T22:43:05.274516Z","iopub.status.idle":"2025-05-23T22:43:05.280871Z","shell.execute_reply.started":"2025-05-23T22:43:05.274495Z","shell.execute_reply":"2025-05-23T22:43:05.280288Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_epochs = 40\nearly_stopping = EarlyStopping(patience=5, verbose=True, path='best_crnn.pth')\nfor epoch in range(num_epochs):\n    print(f\"Epoch {epoch+1}/{num_epochs}\")\n    train_epoch(model, train_loader, optimizer, ctc_loss, device)\n    val_loss = validate_epoch(model, val_loader, ctc_loss, device)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\"Early stopping triggered.\")\n        break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T22:43:06.275860Z","iopub.execute_input":"2025-05-23T22:43:06.276407Z","iopub.status.idle":"2025-05-23T22:43:36.156483Z","shell.execute_reply.started":"2025-05-23T22:43:06.276387Z","shell.execute_reply":"2025-05-23T22:43:36.155540Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Decoding","metadata":{}},{"cell_type":"code","source":"def beam_search_decode(probs, beam_width=5, blank=0):\n    import math\n    from collections import defaultdict\n\n    seq_len, batch_size, nclass = probs.size()\n    decoded_batch = []\n\n    for batch_idx in range(batch_size):\n        beam = [(tuple(), 0.0)]\n\n        for t in range(seq_len):\n            new_beam = defaultdict(lambda: -math.inf)\n            time_step_log_prob = probs[t, batch_idx].cpu().numpy()\n\n            for seq, score in beam:\n                for c in range(nclass):\n                    p = time_step_log_prob[c]\n                    if len(seq) > 0 and c == seq[-1]:\n                        new_seq = seq\n                    else:\n                        new_seq = seq + (c,) if c != blank else seq\n                    new_score = score + p\n                    if new_score > new_beam[new_seq]:\n                        new_beam[new_seq] = new_score\n\n            beam = sorted(new_beam.items(), key=lambda x: x[1], reverse=True)[:beam_width]\n\n        best_seq, best_score = beam[0]\n\n        # Filter blanks and repeated characters here\n        decoded = []\n        prev = None\n        for idx in best_seq:\n            if idx != blank and idx != prev:\n                # Defensive check in case idx_to_char missing key\n                char = idx_to_char.get(idx, '')\n                if char != '':\n                    decoded.append(char)\n            prev = idx\n\n        decoded_str = \"\".join(decoded)\n        decoded_batch.append(decoded_str)\n\n    return decoded_batch\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T22:26:28.291482Z","iopub.execute_input":"2025-05-23T22:26:28.291768Z","iopub.status.idle":"2025-05-23T22:26:28.298805Z","shell.execute_reply.started":"2025-05-23T22:26:28.291738Z","shell.execute_reply":"2025-05-23T22:26:28.298209Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"def clean_decoded_text(text, blank_char=''):\n    \"\"\"\n    Remove duplicates and blanks if any remain.\n    Assumes blank_char is '' (empty string) for blank token.\n    \"\"\"\n    cleaned = []\n    prev_char = None\n    for ch in text:\n        if ch != blank_char and ch != prev_char:\n            cleaned.append(ch)\n        prev_char = ch\n    return ''.join(cleaned)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T22:26:29.547830Z","iopub.execute_input":"2025-05-23T22:26:29.548390Z","iopub.status.idle":"2025-05-23T22:26:29.552031Z","shell.execute_reply.started":"2025-05-23T22:26:29.548371Z","shell.execute_reply":"2025-05-23T22:26:29.551477Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef process_image(image_path):\n    image = Image.open(image_path).convert('L')  # grayscale\n    transform_image = transform(image=np.array(image))\n    image = transform_image['image']\n    if hasattr(image, 'numpy'):\n        old_image = image.numpy()\n\n    # If image is (C, H, W), squeeze or select channel for grayscale\n    if old_image.ndim == 3:\n        old_image = old_image.squeeze()  # for single-channel, or use image[0] if you want\n\n    plt.imshow(old_image, cmap='gray')\n    plt.axis('off')\n    plt.show()\n    return image","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with torch.no_grad():\n    image = process_image('/kaggle/input/test-image/Times-New-Roman-Font.png')\n    image = image.unsqueeze(0).to(device)\n    output = model(image)\n    decoded_texts = beam_search_decode(output, beam_width=10, blank=0)\n    raw_text = decoded_texts\n    cleaned_text = clean_decoded_text(raw_text)\n    print(\"Raw decoded text:\", raw_text)\n    print(\"Cleaned text:\", cleaned_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T22:34:18.345844Z","iopub.execute_input":"2025-05-23T22:34:18.346079Z","iopub.status.idle":"2025-05-23T22:34:18.376914Z","shell.execute_reply.started":"2025-05-23T22:34:18.346064Z","shell.execute_reply":"2025-05-23T22:34:18.376322Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}