{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11426150,"sourceType":"datasetVersion","datasetId":7155269},{"sourceId":409576,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":334578,"modelId":355613},{"sourceId":410112,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":334644,"modelId":355678}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch torchvision torchaudio --quiet\n!pip install albumentations --quiet  # for augmentation (optional)\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nimport torchvision.transforms as transforms\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nimport numpy as np\nfrom PIL import Image\nimport os\nimport string\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-24T16:32:01.200322Z","iopub.execute_input":"2025-05-24T16:32:01.200780Z","iopub.status.idle":"2025-05-24T16:33:27.115081Z","shell.execute_reply.started":"2025-05-24T16:32:01.200747Z","shell.execute_reply":"2025-05-24T16:33:27.114148Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m78.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/albumentations/__init__.py:28: UserWarning: A new version of Albumentations is available: '2.0.7' (you have '2.0.5'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n  check_for_updates()\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Define Vietnamese characters set (you can expand it)\n# Including a blank character '' for CTC at index 0\nlowercase = \"aăâbcdđeêghijklmnoôơpqrstuưvwxyz\" \\\n            \"áàảãạằắẳẵặấầẩẫậéèẻẽẹếềểễệíìỉĩịóòỏõọốồổỗộớờởỡợúùủũụứừửữựýỳỷỹỵ0123456789\"\n\nuppercase = lowercase.upper()\n\nspecial_chars = \"/!@#$%^&*()_+:,.-;?{}[]|~` \"\n\nfull_alphabet = lowercase + uppercase + special_chars\nprint(full_alphabet)\n# Map char to index and vice versa\nchar_to_idx = {char: idx + 1 for idx, char in enumerate(full_alphabet)}  # start at 1; 0 is blank for CTC\nidx_to_char = {idx: char for char, idx in char_to_idx.items()}\n\n# Add blank character at index 0\nidx_to_char[0] = ''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T16:33:27.115918Z","iopub.execute_input":"2025-05-24T16:33:27.116346Z","iopub.status.idle":"2025-05-24T16:33:27.122088Z","shell.execute_reply.started":"2025-05-24T16:33:27.116319Z","shell.execute_reply":"2025-05-24T16:33:27.121271Z"}},"outputs":[{"name":"stdout","text":"aăâbcdđeêghijklmnoôơpqrstuưvwxyzáàảãạằắẳẵặấầẩẫậéèẻẽẹếềểễệíìỉĩịóòỏõọốồổỗộớờởỡợúùủũụứừửữựýỳỷỹỵ0123456789AĂÂBCDĐEÊGHIJKLMNOÔƠPQRSTUƯVWXYZÁÀẢÃẠẰẮẲẴẶẤẦẨẪẬÉÈẺẼẸẾỀỂỄỆÍÌỈĨỊÓÒỎÕỌỐỒỔỖỘỚỜỞỠỢÚÙỦŨỤỨỪỬỮỰÝỲỶỸỴ0123456789/!@#$%^&*()_+:,.-;?{}[]|~` \n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\n\nclass VietnameseOCRDataset(Dataset):\n    def __init__(self, img_dir, labels_csv, transform=None):\n        self.img_dir = img_dir\n        self.transform = transform\n        df = pd.read_csv(labels_csv, encoding='utf-8')\n        self.samples = list(zip(df['image_name'], df['text']))\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        img_name, label = self.samples[idx]\n        img_path = os.path.join(self.img_dir, img_name)\n        image = Image.open(img_path).convert('L')  # grayscale\n    \n        if self.transform:\n            augmented = self.transform(image=np.array(image))  # pass as named argument\n            image = augmented['image']                        # get transformed image tensor\n    \n        # Encode label string to list of indices\n        label_idx = [char_to_idx[char] for char in label if char in char_to_idx]\n    \n        return image, torch.tensor(label_idx, dtype=torch.long)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T16:33:27.123981Z","iopub.execute_input":"2025-05-24T16:33:27.124178Z","iopub.status.idle":"2025-05-24T16:33:27.421247Z","shell.execute_reply.started":"2025-05-24T16:33:27.124162Z","shell.execute_reply":"2025-05-24T16:33:27.420679Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"transform = A.Compose([\n    A.Resize(32, 2048),  # height fixed to 32, width 128 (adjust as needed)\n    A.Normalize(mean=(0.5,), std=(0.5,)),\n    ToTensorV2(),\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T16:33:27.424544Z","iopub.execute_input":"2025-05-24T16:33:27.424816Z","iopub.status.idle":"2025-05-24T16:33:27.432655Z","shell.execute_reply.started":"2025-05-24T16:33:27.424791Z","shell.execute_reply":"2025-05-24T16:33:27.432070Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"train_dataset = VietnameseOCRDataset('/kaggle/input/vaipe-crops/vaipe_crops/train', '/kaggle/input/vaipe-crops/vaipe_crops/train.csv', transform=transform)\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=lambda x: x)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T16:33:27.433377Z","iopub.execute_input":"2025-05-24T16:33:27.433606Z","iopub.status.idle":"2025-05-24T16:33:27.728469Z","shell.execute_reply.started":"2025-05-24T16:33:27.433580Z","shell.execute_reply":"2025-05-24T16:33:27.727903Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class CRNN(nn.Module):\n    def __init__(self, imgH, nc, nclass, nh):\n        super(CRNN, self).__init__()\n        assert imgH % 16 == 0, \"imgH has to be a multiple of 16\"\n\n        self.cnn = nn.Sequential(\n            nn.Conv2d(nc, 64, 3, 1, 1),  # conv1\n            nn.ReLU(True),\n            nn.MaxPool2d(2, 2),          # 32x128 -> 16x64\n\n            nn.Conv2d(64, 128, 3, 1, 1), # conv2\n            nn.ReLU(True),\n            nn.MaxPool2d(2, 2),          # 16x64 -> 8x32\n\n            nn.Conv2d(128, 256, 3, 1, 1), # conv3\n            nn.ReLU(True),\n            nn.Conv2d(256, 256, 3, 1, 1), # conv4\n            nn.ReLU(True),\n            nn.MaxPool2d((2,2), (2,1), (0,1)), # 8x32 -> 4x33\n\n            nn.Conv2d(256, 512, 3, 1, 1), # conv5\n            nn.BatchNorm2d(512),\n            nn.ReLU(True),\n            nn.Conv2d(512, 512, 3, 1, 1), # conv6\n            nn.BatchNorm2d(512),\n            nn.ReLU(True),\n            nn.MaxPool2d((2,2), (2,1), (0,1)), # 4x33 -> 2x34\n\n            nn.Conv2d(512, 512, 2, 1, 0),  # conv7 kernel=2 no padding\n            nn.ReLU(True)\n        )\n\n        self.rnn = nn.LSTM(\n            input_size=512,\n            hidden_size=nh,\n            num_layers=2,\n            bidirectional=True,\n            batch_first=True\n        )\n\n        self.embedding = nn.Linear(nh * 2, nclass)\n\n    def forward(self, x):\n        # x: (batch, channel=1, height, width)\n        conv = self.cnn(x)  # [batch, 512, 1, width']\n        b, c, h, w = conv.size()\n        assert h == 1, \"height after conv must be 1\"\n        conv = conv.squeeze(2)  # [batch, 512, width]\n        conv = conv.permute(0, 2, 1)  # [batch, width, 512]\n\n        rnn_out, _ = self.rnn(conv)  # [batch, width, nh*2]\n        output = self.embedding(rnn_out)  # [batch, width, nclass]\n\n        # output: logit sequence for CTC loss\n        return output.log_softmax(2)  # for CTC loss: log prob on dim=2\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T16:33:27.729138Z","iopub.execute_input":"2025-05-24T16:33:27.729427Z","iopub.status.idle":"2025-05-24T16:33:27.738060Z","shell.execute_reply.started":"2025-05-24T16:33:27.729401Z","shell.execute_reply":"2025-05-24T16:33:27.737353Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"device = torch.device('cuda')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T16:33:27.738870Z","iopub.execute_input":"2025-05-24T16:33:27.739161Z","iopub.status.idle":"2025-05-24T16:33:27.755917Z","shell.execute_reply.started":"2025-05-24T16:33:27.739142Z","shell.execute_reply":"2025-05-24T16:33:27.755113Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"model = CRNN(imgH=32, nc=1, nclass=len(full_alphabet) + 1, nh=256).to(device)\nctc_loss = nn.CTCLoss(blank=0, zero_infinity=True)\noptimizer = optim.Adam(model.parameters(), lr=0.0005)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T16:33:27.758118Z","iopub.execute_input":"2025-05-24T16:33:27.758415Z","iopub.status.idle":"2025-05-24T16:33:28.109608Z","shell.execute_reply.started":"2025-05-24T16:33:27.758386Z","shell.execute_reply":"2025-05-24T16:33:28.109014Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"model.load_state_dict(torch.load('/kaggle/input/cnn-ocr-2/pytorch/default/4/best_crnn.pth'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T16:33:28.110130Z","iopub.execute_input":"2025-05-24T16:33:28.110325Z","iopub.status.idle":"2025-05-24T16:33:28.847990Z","shell.execute_reply.started":"2025-05-24T16:33:28.110309Z","shell.execute_reply":"2025-05-24T16:33:28.847408Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"def beam_search_decode(probs, beam_width=5, blank=0):\n    import math\n    from collections import defaultdict\n\n    seq_len, batch_size, nclass = probs.size()\n    decoded_batch = []\n\n    for batch_idx in range(batch_size):\n        beam = [(tuple(), 0.0)]\n\n        for t in range(seq_len):\n            new_beam = defaultdict(lambda: -math.inf)\n            time_step_log_prob = probs[t, batch_idx].cpu().numpy()\n\n            for seq, score in beam:\n                for c in range(nclass):\n                    p = time_step_log_prob[c]\n                    if len(seq) > 0 and c == seq[-1]:\n                        new_seq = seq\n                    else:\n                        new_seq = seq + (c,) if c != blank else seq\n                    new_score = score + p\n                    if new_score > new_beam[new_seq]:\n                        new_beam[new_seq] = new_score\n\n            beam = sorted(new_beam.items(), key=lambda x: x[1], reverse=True)[:beam_width]\n\n        best_seq, best_score = beam[0]\n\n        # Filter blanks and repeated characters here\n        decoded = []\n        prev = None\n        for idx in best_seq:\n            if idx != blank and idx != prev:\n                # Defensive check in case idx_to_char missing key\n                char = idx_to_char.get(idx, '')\n                if char != '':\n                    decoded.append(char)\n            prev = idx\n\n        decoded_str = \"\".join(decoded)\n        decoded_batch.append(decoded_str)\n\n    return decoded_batch\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T16:33:28.848721Z","iopub.execute_input":"2025-05-24T16:33:28.848973Z","iopub.status.idle":"2025-05-24T16:33:28.856111Z","shell.execute_reply.started":"2025-05-24T16:33:28.848956Z","shell.execute_reply":"2025-05-24T16:33:28.855457Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def clean_decoded_text(text, blank_char=''):\n    \"\"\"\n    Remove duplicates and blanks if any remain.\n    Assumes blank_char is '' (empty string) for blank token.\n    \"\"\"\n    cleaned = []\n    prev_char = None\n    for ch in text:\n        if ch != blank_char and ch != prev_char:\n            cleaned.append(ch)\n        prev_char = ch\n    return ''.join(cleaned)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T16:33:28.856860Z","iopub.execute_input":"2025-05-24T16:33:28.857095Z","iopub.status.idle":"2025-05-24T16:33:28.872277Z","shell.execute_reply.started":"2025-05-24T16:33:28.857079Z","shell.execute_reply":"2025-05-24T16:33:28.871610Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"with torch.no_grad():\n    sample_img, _ = train_dataset[3]\n    sample_img = sample_img.unsqueeze(0).to(device)\n    output = model(sample_img)\n    decoded_texts = beam_search_decode(output, beam_width=10, blank=0)\n    raw_text = decoded_texts\n    cleaned_text = clean_decoded_text(raw_text)\n    print(\"Raw decoded text:\", raw_text)\n    print(\"Cleaned text:\", cleaned_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T16:34:31.937091Z","iopub.execute_input":"2025-05-24T16:34:31.937776Z","iopub.status.idle":"2025-05-24T16:34:32.301975Z","shell.execute_reply.started":"2025-05-24T16:34:31.937732Z","shell.execute_reply":"2025-05-24T16:34:32.301144Z"}},"outputs":[{"name":"stdout","text":"Raw decoded text: ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'M', '', '', '', 'M', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'H', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'g', 'g', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ' ', ' ', ' ', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'T', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'h', 'h', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'â', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'n', 'n', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ' ', ' ', ' ', '', '', '', '', '', '', '', '', 'n', 'n', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'h', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'i', 'i', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'ệ', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 't', '', '', '', '', '', '', '', '', '', '', '', '', ':', '', '', '', '', '', '', '', '', '', '', '', '', '']\nCleaned text: MMHg Thân nhiệt:\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}